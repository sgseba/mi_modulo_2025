# -*- coding: utf-8 -*-
"""Mis clases.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wCVrN7JmF97Z6ozQku9_82b8PMWNg0Cp

#1. Clase GeneradoraDeDatos
"""

class GeneradoraDeDatos:
  # Recibe n entero = cantidad de datos a generar.
  def __init__(self, n):
    self.n = n

  def genera_datos_dist_uniforme(self):
    # Usa n, devuelve array de datos con distrib uniforme.
    return np.random.uniform(self.n)

  def genera_datos_dist_norm(self, media=0, desvio=1):
    # Usa n. Puede recibir media, desvío. Devuelve array de datos con distrib N(media,desvio).
    return np.random.normal(media, desvio, self.n)

  def genera_datos_BS(self):
    # Usa n, devuelve array de datos_BS ~ N(0,1).
    y = 0.5 + np.random.normal(0,1,self.n)
    for j in range(5):
      y += 0.1 * np.random.normal(0.5*j - 1, 0.1,self.n)
    return y

  def densidad_normal(self, x, media=0, desvio=1):
    # Recibe x, array de grilla. Puede recibir media, desvío.
    # Devuelve la densidad de probabilidad N(0,1).
    return norm.pdf(x,media,desvio)

"""#2. Clase Estimacion"""

class Estimacion:
  def __init__(self, datos):
      self.datos = datos

  def genera_histograma(self, h):
    # Genera histograma.
    # Necesita datos y h (ventana).
    # Retorna puntos separadores de bins y recuento de cada bin.

    bins = np.arange(min(self.datos), max(self.datos)+h,h)
    histo = np.zeros(len(bins)-1)
    for dato in self.datos:
      for i in range(len(histo)):
        if bins[i] <= dato < bins[i+1]:
          histo[i] +=1
          break
    histo /= (len(self.datos) * h)
    return bins, histo

  def evalua_histograma(self, h, x):
      # Utiliza genera_histograma, a partir de datos y h,  como función de densidad para estimar las probabilidades en x.

      bins, frec = self.genera_histograma(h)
      estim_hist = np.zeros(len(x))
      for j in range(len(x)):
        for i in range(len(bins)-1):
            if bins[i] <= x[j] < bins[i+1]:
              estim_hist[j] = frec[i]
              break
      return estim_hist


  def caracteristica (self,x,int_inf, int_sup):
    # Recibe x.
    # Devuelve 1 si x en (,), 0 si no.
    return 1*((x>int_inf) & (x<int_sup))

  def kernel_gaussiano(self,x):
    # Kernel gaussiano estándar
    # Recibe x, devuelve K(x)
    valor_kernel_gaussiano = (1/np.sqrt(2*np.pi)) * sum(np.exp(-x**2/2))
    return valor_kernel_gaussiano

  def kernel_uniforme(self,x):
    # Kernel uniforme estándar
    # Recibe x, devuelve K(x)
    valor_kernel_uniforme = 0.5*(sum(self.caracteristica(x,-1,1)))
    return valor_kernel_uniforme

  def kernel_cuadra(self,x):
    # Kernel cuadrático estándar
    # Recibe x, devuelve K(x)
    return 0.75*(1-x**2) * self.caracteristica(x, -1, 1)

  def kernel_triangu(self,x):
    # Kernel triangular estándar
    # Recibe x, devuelve K(x)
    return (1+x)*self.caracteristica(x,-1,0) + (1-x)*self.caracteristica(x,0,1)

  def mi_densidad(self, x, h, kernel):
    # Recibe x: Puntos en los que se evaluará la densidad.
    # Recibe h: Ancho de la ventana (bandwidth).
    # Recibe kernel: string del kernel a usar.
    # Usa: self.datos: Datos

    n = len(x)
    density = np.zeros_like(x)
    for i in range(n):

        if kernel in ['uniforme', 'Uniforme', 'KUnif']:
          density[i] = np.sum(self.kernel_uniforme((self.datos - x[i])/h))

        elif kernel in ['gauss', 'Gauss', 'gaussiano', 'Gaussiano', 'KGauss']:
          density[i] = np.sum(self.kernel_gaussiano((self.datos - x[i])/h))

        elif kernel in ['cuadra', 'Cuadra', 'KCuadra', 'Cuadra','Cuadrado', 'KCuadrado']:
          density[i] = np.sum(self.kernel_cuadra((self.datos - x[i])/h))

        elif kernel in ['triangu', 'Triangu', 'KTriangu', 'triangular', 'Triangular', 'KTriangular']:
          density[i] = np.sum(self.kernel_triangu((self.datos - x[i])/h))

    return density / (len(self.datos)*h)

  def miqqplot(self):

    cuantiles = np.arange(1,11)/11
    cuantiles_teoricos = norm.ppf(cuantiles)

    data_ord = np.zeros_like(cuantiles)
    for i in range(len(cuantiles)):
      data_ord[i] = np.sort(self.datos)[int(cuantiles[i]*len(self.datos)) - 1]
    cuantiles_muestrales = (data_ord - np.mean(self.datos))/np.std(self.datos)

    plt.scatter(cuantiles_teoricos, cuantiles_muestrales, color='blue', marker='o')
    plt.xlabel('Cuantiles teóricos')
    plt.ylabel('Cuantiles muestrales')
    plt.plot(cuantiles_teoricos,cuantiles_teoricos , linestyle='-', color='red')
    plt.show()

  def qqpplot_teorico(self):
    sm.qqplot((self.datos-np.mean(self.datos))/np.std(self.datos), line='45')
    plt.title('QQ Plot')
    plt.show()

"""#3. Clase RegresionLinealSimple

## Versión reducida, EN USO.
"""

# VERSION REDUCIDA 27-04-25
import numpy as np
from scipy.stats import norm
from scipy.stats import t
import matplotlib.pyplot as plt
import statsmodels.api as sm

class RegresionLinealSimple():

   # Recibe los datos como 2 ARRAY SIN ENCABEZADO y los convierte en array de numpy
    # x variable predictora=keys(), y variable respuesta=values().
    # Calcula b0 y b1 en init
    # Método grafico_scatter_y_lineal: genera la recta con b0 y b1, y grafica datos y recta.
    # Método estimar_y: genera los datos estimados a partir de x.
    # Método calcular_residuos: y_estim - y_dato
    # Método calcular_varianza_residuos: da eso.
    # Método t_obs: calcula el t_obs para b1.
    # Método calcula_region_rechazo: dado alfa, calcula el t_critico para la región de rechazo de H0: b1=0.

  def __init__(self,datos_x, datos_y, label_x='x', label_y='y'):
      self.datos_x = datos_x
      self.datos_y = datos_y
      self.datos_x_label = label_x
      self.datos_y_label = label_y

  def calcular_regresion(self):
      # Calcula los atributos esenciales del modelo de regresión lineal simple.
        #- Parámetros b0,b1
        #- y_estimada por la regresión
        #- residuos: datos originales - estimados
        #- varianza_residuos
        #- varianza_b1
        #- varianza_b0
        #- t_critico_05: valor de la distribución teórica t, con (n-2) gl, alfa=0.05
        #- t_obs_b1: valor de la distribución t para el valor observado de b1.
        #- p_valor_b1: p-valor para el valor t_obs_b1, bilateral, alfa=0.05 (0.025-0.975)
      # Métodos:
        #- Calcular graficar dispersión de datos originales y recta de ajuste
        #- Calcular t_obs_b1_otro para testear H0: b1=otro_valor, H1: !=otro_valor  (por DEFECTO=0)
        #- Calcular t_obs_b0_otro para testear H0: b0=otro_valor, H1: !=otro_valor  (por DEFECTO=0)
        #- Calcular t_critico bilateral para otro alfa (por DEFECTO=0.05)
        #- Calcular intervalo de confianza para la media de y, dado x0.
        #- Calcular intervalo de predicción puntual para y, dado x0.

      ## ATRIBUTOS
      # Estimadores de los parámetros: b0_est y b1_est
      self.b1_est = np.sum((self.datos_x - np.mean(self.datos_x)) * (self.datos_y - np.mean(self.datos_y))) / np.sum((self.datos_x -  np.mean(self.datos_x))**2)
      self.b0_est = np.mean(self.datos_y) - self.b1_est * np.mean(self.datos_x)

      # Predichos y_estimada
      self.y_estimada = self.b0_est + self.b1_est * self.datos_x

      # Residuos: y-y_estimada
      self.residuos = self.datos_y - self.y_estimada

      # Estimador de varianza de residuos sigma^2
      self.sigma2_est = np.sum(self.residuos**2)/(len(self.datos_x)-2)

      # Estimador de varianza b1_estimador
      self.varianza_b1_est = self.sigma2_est / np.sum((self.datos_x - np.mean(self.datos_x))**2)

      # Estimador de varianza b0_estimador
      self.varianza_b0_est = self.sigma2_est * np.sum(self.datos_x**2)/ (len(self.datos_x) * np.sum((self.datos_x - np.mean(self.datos_x))**2))

      # Calcula t_critico_05 para datos_x, alfa =0.05.
      alfa = 0.05
      self.t_critico_05 = t.ppf(1-(alfa/2),len(self.datos_x)-2)

      # Calcula t_obs_b1 para H0: b1=0, H1: !=0.
      self.t_obs_b1 = self.b1_est/np.sqrt(self.varianza_b1_est)

      # Calcula t_obs_b0 para H0: b0=0, H1: !=0.
      self.t_obs_b0 = self.b0_est/np.sqrt(self.varianza_b0_est)

      # Calcula el p_valor para H0: b1 = 0, H1: != 0.
      self.p_valor_b1= 2*(1-t.cdf(np.abs(self.t_obs_b1),len(self.datos_x)-2))

      # Creo resumen diccionario
      self.resumen = {'b0_est': self.b0_est,
                      'b1_est': self.b1_est,
                      'y_est': self.y_estimada,
                      'residuos': self.residuos,
                      'varianza_residuos_est': self.sigma2_est,
                      'varianza_b0_est': self.varianza_b0_est,
                      'desvio_b0_est': np.sqrt(self.varianza_b0_est),
                      'varianza_b_est': self.varianza_b1_est,
                      'desvio_b1_est': np.sqrt(self.varianza_b1_est),
                      't_critico_05': self.t_critico_05,
                      't_obs_b0': self.t_obs_b0,
                      't_obs_b1': self.t_obs_b1,
                      'p_valor_b1': self.p_valor_b1}
      return self.resumen

  def imprimir_resumen(self):
    print('Resumen de la regresión lineal simple:')
    print('--------------------------------------')
    for key, value in self.resumen.items():
      if key not in ['y_est', 'residuos']:
        print(f'{key}: {value}')
    print('--------------------------------------')

  def grafico_scatter_y_lineal(self):
      # Grafica los puntos de los datos originales y la recta de mejor ajuste por mínimos cuadrados obtenida por regresión lineal simple.
      self.calcular_regresion()
      plt.figure(figsize=(10, 6))
      plt.scatter(self.datos_x, self.datos_y, label="Datos")
      plt.plot(self.datos_x, self.y_estimada, label="Recta Estimada", linestyle="--", color="green")
      plt.xlabel(self.datos_x_label)
      plt.ylabel(self.datos_y_label)
      plt.title("Gráfico de Dispersión con Recta de Mejor Ajuste")
      plt.legend()

  #def estimar_y(self):
      # Calcula y_estimada
      # self.calcular_parametros(): no, xq los puse en el init.

   #   return self.y_estimada

  #def calcular_residuos(self):
      # Calcula los residuos: y_dato - y_estimada
   #   self.estimar_y()
   #   return self.datos_y - self.y_estimada

  #def calcular_varianza_residuos(self):
   #   # Calcula varianza_residuos
   #   return np.sum(self.calcular_residuos()**2)/(len(self.datos_x)-2)

  def calcular_t_obs_b1_otro(self, b1_propuesto = 0):
    # Calcula t_obs_b1_otro para b1 bajo H0: b1 = b1_propuesto, por DEFECTO = 0.
    self.t_obs_b1_otro = (self.b1_est-b1_propuesto)/np.sqrt(self.sigma2_est)
    return self.t_obs_b1_otro

  def calcular_t_obs_b0_otro(self,b0_propuesto = 0):
    # Calcula t_obs_b0_otro para b0 bajo H1: b0 = b0_propuesto, por DEFECTO = 0.
    self.t_obs_b0_otro = (self.b0_est-b0_propuesto)/np.sqrt(self.sigma2_est)
    return self.t_obs_b0_otro

  def calcular_t_critico(self,alfa=0.05):
    # Calcula el t_critico (teórico = tabulado) para dar el intervalo BILATERAL (alfa/2 para cada lado). Por DEFECTO = 0.05.
    self.t_critico = t.ppf(1-(alfa/2),len(self.datos_x)-2)
    print(f'Región RECHAZO: (-∞,- {self.t_critico:.4f}) ∪ ({self.t_critico:.4f},∞)')
    return self.t_critico

  def calcular_intervalo_confianza(self,x_0, alfa=0.05, pred=False):
    # Calcula intervalo de confianza para mu_y|x=x_0, dado un valor puntual de x
    # Si pred=True, calcula intervalo de predicción puntual para y|x=x0.
    # Necesito coviaranza de (b0_est, b1_est).
    t_critico = t.ppf(1-(alfa/2),len(self.datos_x)-2)
    cov_01=-np.mean(x)*self.sigma2_est/sum((x-np.mean(x))**2)
    SE2_conf_est=self.varianza_b0_est+(x_0**2)*self.varianza_b1_est+2*x_0*cov_01
    y0=self.b0_est+self.b1_est*x_0

    intervalo = [y0-t_critico*np.sqrt(SE2_conf_est),y0+t_critico*np.sqrt(SE2_conf_est)]

    print(f'Valor predicho para y|x={x_0}: {y0}')

    if pred:
      SE2_pred_est=SE2_conf_est + self.sigma2_est
      intervalo = [y0-t_critico*np.sqrt(SE2_pred_est),y0+t_critico*np.sqrt(SE2_pred_est)]
      print(f'Intervalo de predicción para y|x={x_0}: [{intervalo[0], {intervalo[1]}}')
    else:
        print(f'Intervalo de confianza para mu_y|x={x_0}: [{intervalo[0], {intervalo[1]}}')

    return intervalo


 # def calcular_p_valor(self):
    # Calcula el p_valor para H0: b1 = 0.
 #   return 2*(1-t.cdf(np.abs(self.t_obs),len(self.datos_x)-2))

## Usando el paquete sm
  # Métodos:
    # -calcular_regresion_sm
    # -calcular_varianza_error_sm : mse_resid (SCE/gl)
    # -calcular_r2_sm
    # -calcular_se_betas_sm
    # -calcular_p_valores_sm
    # -calcular_int_conf_betas_sm
    # -calcular_t_obs_b0_sm
    # -calcular_t_obs_b1_sm
    # -calcular_int_conf_pred_nuevo_x_sm

  def calcular_regresion_sm(self):
    self.X = sm.add_constant(self.datos_x)
    modelo = sm.OLS(self.datos_y, self.X)
    self.resultado = modelo.fit()
    print(self.resultado.summary())

    # Calcula b0,b1
    self.betas_sm = self.resultado.params
    print("Parámetros de la regresión:")
    print("b0:", self.betas_sm.values[0])
    print("b1:", self.betas_sm.values[1])

    # Calcula la varianza del error
    self.varianza_error_sm = self.resultado.mse_resid
    print("Varianza del error:", self.varianza_error_sm)

    # Calcula el coeficiente de determinación
    self.r2_sm = self.resultado.rsquared
    print("Coeficiente de determinación:", self.r2_sm)

    # Calcula los desvíos standard para b0 y b1
    self.se_betas_sm = self.resultado.bse
    print("Errores estándar de los coeficientes:")
    print("SE(b0.est):", self.se_betas_sm.values[0])
    print("SE(b1.est):", self.se_betas_sm.values[1])

    # Calcula el p_valor para H0: bi = 0.
    self.p_valores_sm = self.resultado.pvalues
    print("P-valores de los coeficientes:")
    print("P-valor(b0.est):", self.p_valores_sm.values[0])
    print("P-valor(b1.est):", self.p_valores_sm.values[1])

  def calcular_t_obs_b0_sm(self, b0_propuesto = 0):
    # Calcula t_obs para b0 bajo H0: b0 = b0_propuesto, por DEFECTO = 0.
    self.t_obs_b0_sm = (self.betas_sm.values[0]-b0_propuesto)/self.se_betas_sm.values[0]
    print("t_obs(b0.est):", self.t_obs_b0_sm)
    #return self.t_obs_b0_sm

  def calcular_t_obs_b1_sm(self, b1_propuesto = 0):
    # Calcula t_obs para b1 bajo H0: b1 = b1_propuesto, por DEFECTO = 0.
    self.t_obs_b1_sm = (self.betas_sm.values[1]-b1_propuesto)/self.se_betas_sm.values[1]
    print("t_obs(b1.est):", self.t_obs_b1_sm)
    #return self.t_obs_b1_sm

  def calcular_int_conf_betas_sm(self, alfa=0.05):
    # Calcula el intervalo de confianza para b0 y b1
    self.int_conf_betas_sm = self.resultado.conf_int(alpha=alfa)
    print("Intervalo de confianza para b0.est:", self.int_conf_betas_sm.values[0])
    print("Intervalo de confianza para b1.est:", self.int_conf_betas_sm.values[1])

  def calcular_int_conf_pred_nuevo_x_sm(self, x_new, alfa=0.05, int_pred=False):
    # Nuevo punto de predicción
    self.x_new = x_new

    # Respuesta estimada para x_new
    self.y_new = self.betas_sm.values[0] + self.betas_sm.values[1] * self.x_new
    print("Respuesta estimada para x_new:", self.y_new)

    # Crear la matriz de diseño con el nuevo punto de predicción
    X_new = sm.add_constant(np.array([[1, self.x_new]]))

    # Obtener el intervalo de predicción para el nuevo punto
    prediccion = self.resultado.get_prediction(X_new)
    self.int_conf = prediccion.conf_int(alpha =alfa)
    print("Intervalo de confianza para la respuesta MEDIA a UN valor nuevo:", self.int_conf)
    if int_pred:
      self.int_pred = prediccion.conf_int(obs=True, alpha=alfa)
      print("Intervalo de predicción para la respuesta a UN valor nuevo:", self.int_pred)

"""#### Versión Reducida. Funciona, pero la mejoré."""

## VERSIÓN REDUCIDA
import numpy as np
from scipy.stats import norm
from scipy.stats import t
import matplotlib.pyplot as plt
import statsmodels.api as sm

class RegresionLinealSimple():

   # Recibe los datos como 2 ARRAY SIN ENCABEZADO y los convierte en array de numpy
    # x variable predictora=keys(), y variable respuesta=values().
    # Calcula b0 y b1 en init
    # Método grafico_scatter_y_lineal: genera la recta con b0 y b1, y grafica datos y recta.
    # Método estimar_y: genera los datos estimados a partir de x.
    # Método calcular_residuos: y_estim - y_dato
    # Método calcular_varianza_residuos: da eso.
    # Método t_obs: calcula el t_obs para b1.
    # Método calcula_region_rechazo: dado alfa, calcula el t_critico para la región de rechazo de H0: b1=0.

  def __init__(self,datos_x, datos_y, label_x='x', label_y='y'):
      self.datos_x = datos_x
      self.datos_y = datos_y
      self.datos_x_label = label_x
      self.datos_y_label = label_y

      # Calcula b0 y b1
      self.b1 = np.sum((self.datos_x - np.mean(self.datos_x)) * (self.datos_y - np.mean(self.datos_y))) / np.sum((self.datos_x -  np.mean(self.datos_x))**2)
      self.b0 = np.mean(self.datos_y) - self.b1 * np.mean(self.datos_x)

  def grafico_scatter_y_lineal(self):
      # Calcula la y_estimada para graficar datos reales e y_estimada
      # self.calcular_parametros()
      y_estimada = self.b0 + self.b1 * self.datos_x
      plt.scatter(self.datos_x, self.datos_y, label="Datos")
      plt.plot(self.datos_x, y_estimada, label="Recta Estimada", linestyle="--", color="green")
      plt.xlabel(self.datos_x_label)              #("Años de Experiencia")
      plt.ylabel(self.datos_y_label)              #("Salario")
      plt.legend()

  def estimar_y(self):
      # Calcula y_estimada
      # self.calcular_parametros(): no, xq los puse en el init.
      self.y_estimada = self.b0 + self.b1 * self.datos_x
      return self.y_estimada

  def calcular_residuos(self):
      # Calcula los residuos: y_dato - y_estimada
      self.estimar_y()
      return self.datos_y - self.y_estimada

  def calcular_varianza_residuos(self):
      # Calcula varianza_residuos
      self.calcular_residuos()
      return np.sum(self.calcular_residuos()**2)/(len(self.datos_x)-2)

  def calcular_t_obs(self, b1_propuesto = 0):
    # Calcula t_obs para b1 bajo H0: b1 = b1_propuesto, por DEFECTO = 0.
    # lo puse así para que calcule b0 y b1 antes, así puedo hacer la sgte linea (ya no es necesario, xq puse b0 b1 en init, pero lo dejo x claridad)
    varianza_residuos = self.calcular_varianza_residuos()
    self.t_obs = (self.b1-b1_propuesto)/np.sqrt(varianza_residuos)
    return self.t_obs

  def calcular_t_critico(self,alfa=0.05):
    # Calcula el t_critico (teórico = tabulado) para dar el intervalo BILATERAL (alfa/2 para cada lado). Por DEFECTO = 0.05.
    self.t_critico = t.ppf(1-(alfa/2),len(self.datos_x)-2)
    print(f'Región RECHAZO: (-∞,- {self.t_critico:.4f}) ∪ ({self.t_critico:.4f},∞)')
    print(f't_critico: {self.t_critico}')
    #return self.t_critico

  def calcular_p_valor(self):
    # Calcula el p_valor para H0: b1 = 0.
    self.calcular_t_obs()
    return 2*(1-t.cdf(np.abs(self.t_obs),len(self.datos_x)-2))

## Usando el paquete sm
  # Métodos:
    # -calcular_regresion_sm
    # -calcular_varianza_error_sm : mse_resid (SCE/gl)
    # -calcular_r2_sm
    # -calcular_se_betas_sm
    # -calcular_p_valores_sm
    # -calcular_int_conf_betas_sm
    # -calcular_t_obs_b0_sm
    # -calcular_t_obs_b1_sm
    # -calcular_int_conf_pred_nuevo_x_sm

  def calcular_regresion_sm(self):
    self.X = sm.add_constant(self.datos_x)
    modelo = sm.OLS(self.datos_y, self.X)
    self.resultado = modelo.fit()
    print(self.resultado.summary())

    # Calcula b0,b1
    self.betas_sm = self.resultado.params
    print("Parámetros de la regresión:")
    print("b0:", self.betas_sm.values[0])
    print("b1:", self.betas_sm.values[1])

    # Calcula la varianza del error
    self.varianza_error_sm = self.resultado.mse_resid
    print("Varianza del error:", self.varianza_error_sm)

    # Calcula el coeficiente de determinación
    self.r2_sm = self.resultado.rsquared
    print("Coeficiente de determinación:", self.r2_sm)

    # Calcula los desvíos standard para b0 y b1
    self.se_betas_sm = self.resultado.bse
    print("Errores estándar de los coeficientes:")
    print("SE(b0.est):", self.se_betas_sm.values[0])
    print("SE(b1.est):", self.se_betas_sm.values[1])

    # Calcula el p_valor para H0: bi = 0.
    self.p_valores_sm = self.resultado.pvalues
    print("P-valores de los coeficientes:")
    print("P-valor(b0.est):", self.p_valores_sm.values[0])
    print("P-valor(b1.est):", self.p_valores_sm.values[1])

  def calcular_t_obs_b0_sm(self, b0_propuesto = 0):
    # Calcula t_obs para b0 bajo H0: b0 = b0_propuesto, por DEFECTO = 0.
    self.t_obs_b0_sm = (self.betas_sm.values[0]-b0_propuesto)/self.se_betas_sm.values[0]
    print("t_obs(b0.est):", self.t_obs_b0_sm)
    #return self.t_obs_b0_sm

  def calcular_t_obs_b1_sm(self, b1_propuesto = 0):
    # Calcula t_obs para b1 bajo H0: b1 = b1_propuesto, por DEFECTO = 0.
    self.t_obs_b1_sm = (self.betas_sm.values[1]-b1_propuesto)/self.se_betas_sm.values[1]
    print("t_obs(b1.est):", self.t_obs_b1_sm)
    #return self.t_obs_b1_sm

  def calcular_int_conf_betas_sm(self, alfa=0.05):
    # Calcula el intervalo de confianza para b0 y b1
    self.int_conf_betas_sm = self.resultado.conf_int(alpha=alfa)
    print("Intervalo de confianza para b0.est:", self.int_conf_betas_sm.values[0])
    print("Intervalo de confianza para b1.est:", self.int_conf_betas_sm.values[1])

  def calcular_int_conf_pred_nuevo_x_sm(self, x_new, alfa=0.05, int_pred=False):
    # Nuevo punto de predicción
    self.x_new = x_new

    # Respuesta estimada para x_new
    self.y_new = self.betas_sm.values[0] + self.betas_sm.values[1] * self.x_new
    print("Respuesta estimada para x_new:", self.y_new)

    # Crear la matriz de diseño con el nuevo punto de predicción
    X_new = sm.add_constant(np.array([[1, self.x_new]]))

    # Obtener el intervalo de predicción para el nuevo punto
    prediccion = self.resultado.get_prediction(X_new)
    self.int_conf = prediccion.conf_int(alpha =alfa)
    print("Intervalo de confianza para la respuesta MEDIA a UN valor nuevo:", self.int_conf)
    if int_pred:
      self.int_pred = prediccion.conf_int(obs=True, alpha=alfa)
      print("Intervalo de predicción para la respuesta a UN valor nuevo:", self.int_pred)

"""### VERSIÓN EN USO"""

import numpy as np
from scipy.stats import norm
from scipy.stats import t
import matplotlib.pyplot as plt
import statsmodels.api as sm

class RegresionLinealSimple():

   # Recibe los datos como 2 ARRAY SIN ENCABEZADO y los convierte en array de numpy
    # x variable predictora=keys(), y variable respuesta=values().
    # Calcula b0 y b1 en init
    # Método grafico_scatter_y_lineal: genera la recta con b0 y b1, y grafica datos y recta.
    # Método estimar_y: genera los datos estimados a partir de x.
    # Método calcular_residuos: y_estim - y_dato
    # Método calcular_varianza_residuos: da eso.
    # Método t_obs: calcula el t_obs para b1.
    # Método calcula_region_rechazo: dado alfa, calcula el t_critico para la región de rechazo de H0: b1=0.

  def __init__(self,datos_x, datos_y, label_x='x', label_y='y'):
      self.datos_x = datos_x
      self.datos_y = datos_y
      self.datos_x_label = label_x
      self.datos_y_label = label_y

      # Calcula b0 y b1
      self.b1 = np.sum((self.datos_x - np.mean(self.datos_x)) * (self.datos_y - np.mean(self.datos_y))) / np.sum((self.datos_x -  np.mean(self.datos_x))**2)
      self.b0 = np.mean(self.datos_y) - self.b1 * np.mean(self.datos_x)

  def grafico_scatter_y_lineal(self):
      # Calcula la y_estimada para graficar datos reales e y_estimada
      # self.calcular_parametros()
      y_estimada = self.b0 + self.b1 * self.datos_x
      plt.scatter(self.datos_x, self.datos_y, label="Datos")
      plt.plot(self.datos_x, y_estimada, label="Recta Estimada", linestyle="--", color="green")
      plt.xlabel(self.datos_x_label)              #("Años de Experiencia")
      plt.ylabel(self.datos_y_label)              #("Salario")
      plt.legend()

  def estimar_y(self):
      # Calcula y_estimada
      # self.calcular_parametros(): no, xq los puse en el init.
      self.y_estimada = self.b0 + self.b1 * self.datos_x
      return self.y_estimada

  def calcular_residuos(self):
      # Calcula los residuos: y_dato - y_estimada
      self.estimar_y()
      return self.datos_y - self.y_estimada

  def calcular_varianza_residuos(self):
      # Calcula varianza_residuos
      self.calcular_residuos()
      return np.sum(self.calcular_residuos()**2)/(len(self.datos_x)-2)

  def calcular_t_obs(self, b1_propuesto = 0):
    # Calcula t_obs para b1 bajo H0: b1 = b1_propuesto, por DEFECTO = 0.
    # lo puse así para que calcule b0 y b1 antes, así puedo hacer la sgte linea (ya no es necesario, xq puse b0 b1 en init, pero lo dejo x claridad)
    varianza_residuos = self.calcular_varianza_residuos()
    self.t_obs = (self.b1-b1_propuesto)/np.sqrt(varianza_residuos)
    return self.t_obs

  def calcular_t_critico(self,alfa=0.05):
    # Calcula el t_critico (teórico = tabulado) para dar el intervalo BILATERAL (alfa/2 para cada lado). Por DEFECTO = 0.05.
    self.t_critico = t.ppf(1-(alfa/2),len(self.datos_x)-2)
    print(f'Región RECHAZO: (-∞,- {self.t_critico:.4f}) ∪ ({self.t_critico:.4f},∞)')
    return self.t_critico

  def calcular_p_valor(self):
    # Calcula el p_valor para H0: b1 = 0.
    self.calcular_t_obs()
    return 2*(1-t.cdf(np.abs(self.t_obs),len(self.datos_x)-2))

## Usando el paquete sm
  # Métodos:
    # -calcular_regresion_sm
    # -calcular_varianza_error_sm : mse_resid (SCE/gl)
    # -calcular_r2_sm
    # -calcular_se_betas_sm
    # -calcular_p_valores_sm
    # -calcular_int_conf_betas_sm
    # -calcular_t_obs_b0_sm
    # -calcular_t_obs_b1_sm
    # -calcular_int_conf_pred_nuevo_x_sm

  def calcular_regresion_sm(self):
    self.X = sm.add_constant(self.datos_x)
    modelo = sm.OLS(self.datos_y, self.X)
    self.resultado = modelo.fit()
    print(self.resultado.summary())

    # Calcula b0,b1
    self.betas_sm = self.resultado.params
    print("Parámetros de la regresión:")
    print("b0:", self.betas_sm[0])
    print("b1:", self.betas_sm[1])

  def calcular_varianza_error_sm(self): #Debería ser igual que se_sm
    # Calcula la varianza del error
    self.varianza_error_sm = self.resultado.mse_resid
    print("Varianza del error:", self.varianza_error_sm)

  def calcular_r2_sm(self):
    # Calcula el coeficiente de determinación
    self.r2_sm = self.resultado.rsquared
    print("Coeficiente de determinación:", self.r2_sm)

  def calcular_se_betas_sm(self):
    # Calcula los desvíos standard para b0 y b1
    self.se_betas_sm = self.resultado.bse
    print("Errores estándar de los coeficientes:")
    print("SE(b0.est):", self.se_betas_sm[0])
    print("SE(b1.est):", self.se_betas_sm[1])

  def calcular_p_valores_sm(self):
    # Calcula el p_valor para H0: bi = 0.
    self.p_valores_sm = self.resultado.pvalues
    print("P-valores de los coeficientes:")
    print("P-valor(b0.est):", self.p_valores_sm[0])
    print("P-valor(b1.est):", self.p_valores_sm[1])

  def calcular_int_conf_betas_sm(self, alfa=0.05):
    # Calcula el intervalo de confianza para b0 y b1
    self.int_conf_betas_sm = self.resultado.conf_int(alpha=alfa)
    print("Intervalo de confianza para b0.est:", self.int_conf_betas_sm[0])
    print("Intervalo de confianza para b1.est:", self.int_conf_betas_sm[1])

  def calcular_t_obs_b0_sm(self, b0_propuesto = 0):
    # Calcula t_obs para b0 bajo H0: b0 = b0_propuesto, por DEFECTO = 0.
    self.t_obs_b0_sm = (self.betas_sm[0]-b0_propuesto)/self.se_betas_sm[0]
    print("t_obs(b0.est):", self.t_obs_b0_sm)
    #return self.t_obs_b0_sm

  def calcular_t_obs_b1_sm(self, b1_propuesto = 0):
    # Calcula t_obs para b1 bajo H0: b1 = b1_propuesto, por DEFECTO = 0.
    self.t_obs_b1_sm = (self.betas_sm[1]-b1_propuesto)/self.se_betas_sm[1]
    print("t_obs(b1.est):", self.t_obs_b1_sm)
    #return self.t_obs_b1_sm

  def calcular_int_conf_pred_nuevo_x_sm(self, x_new, alfa=0.05, int_pred=False):
    # Nuevo punto de predicción
    self.x_new = x_new

    # Respuesta estimada para x_new
    self.y_new = self.betas_sm[0] + self.betas_sm[1] * self.x_new
    print("Respuesta estimada para x_new:", self.y_new)

    # Crear la matriz de diseño con el nuevo punto de predicción
    X_new = sm.add_constant(np.array([[1, self.x_new]]))

    # Obtener el intervalo de predicción para el nuevo punto
    prediccion = self.resultado.get_prediction(X_new)
    self.int_conf = prediccion.conf_int(alpha =alfa)
    print("Intervalo de confianza para la respuesta MEDIA a UN valor nuevo:", self.int_conf)
    if int_pred:
      self.int_pred = prediccion.conf_int(obs=True, alpha=alfa)
      print("Intervalo de predicción para la respuesta a UN valor nuevo:", self.int_pred)

"""## Agrego métodos que usan STATSMODELS. Recibe 2 array y 2 encabezados, NO diccionario."""

import numpy as np
from scipy.stats import norm
from scipy.stats import t
import matplotlib.pyplot as plt
import statsmodels.api as sm

class RegresionLinealSimple():

   # Recibe los datos como 2 ARRAY SIN ENCABEZADO y los convierte en array de numpy
    # x variable predictora=keys(), y variable respuesta=values().
    # Calcula b0 y b1 en init
    # Método grafico_scatter_y_lineal: genera la recta con b0 y b1, y grafica datos y recta.
    # Método estimar_y: genera los datos estimados a partir de x.
    # Método calcular_residuos: y_estim - y_dato
    # Método calcular_varianza_residuos: da eso.
    # Método t_obs: calcula el t_obs para b1.
    # Método calcula_region_rechazo: dado alfa, calcula el t_critico para la región de rechazo de H0: b1=0.

  def __init__(self,datos_x, datos_y, label_x='x', label_y='y'):
      self.datos_x = datos_x
      self.datos_y = datos_y
      self.datos_x_label = label_x
      self.datos_y_label = label_y

      # Calcula b0 y b1
      self.b1 = np.sum((self.datos_x - np.mean(self.datos_x)) * (self.datos_y - np.mean(self.datos_y))) / np.sum((self.datos_x -  np.mean(self.datos_x))**2)
      self.b0 = np.mean(self.datos_y) - self.b1 * np.mean(self.datos_x)

  def grafico_scatter_y_lineal(self):
      # Calcula la y_estimada para graficar datos reales e y_estimada
      # self.calcular_parametros()
      y_estimada = self.b0 + self.b1 * self.datos_x
      plt.scatter(self.datos_x, self.datos_y, label="Datos")
      plt.plot(self.datos_x, y_estimada, label="Recta Estimada", linestyle="--", color="green")
      plt.xlabel(self.datos_x_label)              #("Años de Experiencia")
      plt.ylabel(self.datos_y_label)              #("Salario")
      plt.legend()

  def estimar_y(self):
      # Calcula y_estimada
      # self.calcular_parametros(): no, xq los puse en el init.
      self.y_estimada = self.b0 + self.b1 * self.datos_x
      return self.y_estimada

  def calcular_residuos(self):
      # Calcula los residuos: y_dato - y_estimada
      self.estimar_y()
      return self.datos_y - self.y_estimada

  def calcular_varianza_residuos(self):
      # Calcula varianza_residuos
      return np.sum(self.calcular_residuos()**2)/(len(self.datos_x)-2)

  def calcular_t_obs(self, b1_propuesto = 0):
    # Calcula t_obs para b1 bajo H0: b1 = b1_propuesto, por DEFECTO = 0.
    # lo puse así para que calcule b0 y b1 antes, así puedo hacer la sgte linea (ya no es necesario, xq puse b0 b1 en init, pero lo dejo x claridad)
    varianza_residuos = self.calcular_varianza_residuos()
    self.t_obs = (self.b1-b1_propuesto)/np.sqrt(varianza_residuos)
    return self.t_obs

  def calcular_t_critico(self,alfa=0.05):
    # Calcula el t_critico (teórico = tabulado) para dar el intervalo BILATERAL (alfa/2 para cada lado). Por DEFECTO = 0.05.
    self.t_critico = t.ppf(1-(alfa/2),len(self.datos_x)-2)
    print(f'Región RECHAZO: (-∞,- {self.t_critico:.4f}) ∪ ({self.t_critico:.4f},∞)')
    return self.t_critico

  def calcular_p_valor(self):
    # Calcula el p_valor para H0: b1 = 0.
    return 2*(1-t.cdf(np.abs(self.t_obs),len(self.datos_x)-2))

## Usando el paquete sm
  # Métodos:
    # -calcular_regresion_sm
    # -calcular_varianza_error_sm : mse_resid (SCE/gl)
    # -calcular_r2_sm
    # -calcular_se_betas_sm
    # -calcular_p_valores_sm
    # -calcular_int_conf_betas_sm
    # -calcular_t_obs_b0_sm
    # -calcular_t_obs_b1_sm
    # -calcular_int_conf_pred_nuevo_x_sm

  def calcular_regresion_sm(self):
    self.X = sm.add_constant(self.datos_x)
    modelo = sm.OLS(self.datos_y, self.X)
    self.resultado = modelo.fit()
    print(self.resultado.summary())

    # Calcula b0,b1
    self.betas_sm = self.resultado.params
    print("Parámetros de la regresión:")
    print("b0:", self.betas_sm[0])
    print("b1:", self.betas_sm[1])

  def calcular_varianza_error_sm(self): #Debería ser igual que se_sm
    # Calcula la varianza del error
    self.varianza_error_sm = self.resultado.mse_resid
    print("Varianza del error:", self.varianza_error_sm)

  def calcular_r2_sm(self):
    # Calcula el coeficiente de determinación
    self.r2_sm = self.resultado.rsquared
    print("Coeficiente de determinación:", self.r2_sm)

  def calcular_se_betas_sm(self):
    # Calcula los desvíos standard para b0 y b1
    self.se_betas_sm = self.resultado.bse
    print("Errores estándar de los coeficientes:")
    print("SE(b0.est):", self.se_betas_sm[0])
    print("SE(b1.est):", self.se_betas_sm[1])

  def calcular_p_valores_sm(self):
    # Calcula el p_valor para H0: bi = 0.
    self.p_valores_sm = self.resultado.pvalues
    print("P-valores de los coeficientes:")
    print("P-valor(b0.est):", self.p_valores_sm[0])
    print("P-valor(b1.est):", self.p_valores_sm[1])

  def calcular_int_conf_betas_sm(self, alfa=0.05):
    # Calcula el intervalo de confianza para b0 y b1
    self.int_conf_betas_sm = self.resultado.conf_int(alpha=alfa)
    print("Intervalo de confianza para b0.est:", self.int_conf_betas_sm[0])
    print("Intervalo de confianza para b1.est:", self.int_conf_betas_sm[1])

  def calcular_t_obs_b0_sm(self, b0_propuesto = 0):
    # Calcula t_obs para b0 bajo H0: b0 = b0_propuesto, por DEFECTO = 0.
    self.t_obs_b0_sm = (self.betas_sm[0]-b0_propuesto)/self.se_betas_sm[0]
    print("t_obs(b0.est):", self.t_obs_b0_sm)
    #return self.t_obs_b0_sm

  def calcular_t_obs_b1_sm(self, b1_propuesto = 0):
    # Calcula t_obs para b1 bajo H0: b1 = b1_propuesto, por DEFECTO = 0.
    self.t_obs_b1_sm = (self.betas_sm[1]-b1_propuesto)/self.se_betas_sm[1]
    print("t_obs(b1.est):", self.t_obs_b1_sm)
    #return self.t_obs_b1_sm

  def calcular_int_conf_pred_nuevo_x_sm(self, x_new, alfa=0.05, int_pred=False):
    # Nuevo punto de predicción
    self.x_new = x_new

    # Respuesta estimada para x_new
    self.y_new = self.betas_sm[0] + self.betas_sm[1] * self.x_new
    print("Respuesta estimada para x_new:", self.y_new)

    # Crear la matriz de diseño con el nuevo punto de predicción
    X_new = sm.add_constant(np.array([[1, self.x_new]]))

    # Obtener el intervalo de predicción para el nuevo punto
    prediccion = self.resultado.get_prediction(X_new)
    self.int_conf = prediccion.conf_int(alpha =alfa)
    print("Intervalo de confianza para la respuesta MEDIA a UN valor nuevo:", self.int_conf)
    if int_pred:
      self.int_pred = prediccion.conf_int(obs=True, alpha=alfa)
      print("Intervalo de predicción para la respuesta a UN valor nuevo:", self.int_pred)

"""###Versión en DESUSO"""

import numpy as np
from scipy.stats import norm
from scipy.stats import t
import matplotlib.pyplot as plt
import statsmodels.api as sm

class RegresionLinealSimple():

   # Recibe los datos como 2 ARRAY SIN ENCABEZADO y los convierte en array de numpy
    # x variable predictora=keys(), y variable respuesta=values().
    # Calcula b0 y b1 en init
    # Método grafico_scatter_y_lineal: genera la recta con b0 y b1, y grafica datos y recta.
    # Método estimar_y: genera los datos estimados a partir de x.
    # Método calcular_residuos: y_estim - y_dato
    # Método calcular_varianza_residuos: da eso.
    # Método t_obs: calcula el t_obs para b1.
    # Método calcula_region_rechazo: dado alfa, calcula el t_critico para la región de rechazo de H0: b1=0.

  def __init__(self,datos_x, datos_y, label_x='x', label_y='y'):
      self.datos_x = datos_x
      self.datos_y = datos_y
      self.datos_x_label = label_x
      self.datos_y_label = label_y

      # Calcula b0 y b1
      self.b1 = np.sum((self.datos_x - np.mean(self.datos_x)) * (self.datos_y - np.mean(self.datos_y))) / np.sum((self.datos_x -  np.mean(self.datos_x))**2)
      self.b0 = np.mean(self.datos_y) - self.b1 * np.mean(self.datos_x)

  def grafico_scatter_y_lineal(self):
      # Calcula la y_estimada para graficar datos reales e y_estimada
      # self.calcular_parametros()
      y_estimada = self.b0 + self.b1 * self.datos_x
      plt.scatter(self.datos_x, self.datos_y, label="Datos")
      plt.plot(self.datos_x, y_estimada, label="Recta Estimada", linestyle="--", color="green")
      plt.xlabel(self.datos_x_label)              #("Años de Experiencia")
      plt.ylabel(self.datos_y_label)              #("Salario")
      plt.legend()

  def estimar_y(self):
      # Calcula y_estimada
      # self.calcular_parametros(): no, xq los puse en el init.
      self.y_estimada = self.b0 + self.b1 * self.datos_x
      return self.y_estimada

  def calcular_residuos(self):
      # Calcula los residuos: y_dato - y_estimada
      self.estimar_y()
      return self.datos_y - self.y_estimada

  def calcular_varianza_residuos(self):
      # Calcula varianza_residuos
      return np.sum(self.calcular_residuos()**2)/(len(self.datos_x)-2)

  def calcular_t_obs(self, b1_propuesto = 0):
    # Calcula t_obs para b1 bajo H0: b1 = b1_propuesto, por DEFECTO = 0.
    # lo puse así para que calcule b0 y b1 antes, así puedo hacer la sgte linea (ya no es necesario, xq puse b0 b1 en init, pero lo dejo x claridad)
    varianza_residuos = self.calcular_varianza_residuos()
    self.t_obs = (self.b1-b1_propuesto)/np.sqrt(varianza_residuos)
    return self.t_obs

  def calcular_t_critico(self,alfa=0.05):
    # Calcula el t_critico (teórico = tabulado) para dar el intervalo BILATERAL (alfa/2 para cada lado). Por DEFECTO = 0.05.
    self.t_critico = t.ppf(1-(alfa/2),len(self.datos_x)-2)
    print(f'Región RECHAZO: (-∞,- {self.t_critico:.4f}) ∪ ({self.t_critico:.4f},∞)')
    return self.t_critico

  def calcular_p_valor(self):
    # Calcula el p_valor para H0: b1 = 0.
    return 2*(1-t.cdf(np.abs(self.t_obs),len(self.datos_x)-2))

## Usando el paquete sm
  # Métodos:
    # -calcular_regresion_sm
    # -calcular_varianza_error_sm : mse_resid (SCE/gl)
    # -calcular_r2_sm
    # -calcular_se_betas_sm
    # -calcular_p_valores_sm
    # -calcular_int_conf_betas_sm
    # -calcular_t_obs_b0_sm
    # -calcular_t_obs_b1_sm
    # -calcular_int_conf_pred_nuevo_x_sm

  def calcular_regresion_sm(self):
    X = sm.add_constant(self.datos_x)
    modelo = sm.OLS(self.datos_y, X)
    self.resultado = modelo.fit()
    print(self.resultado.summary())

    # Calcula b0,b1
    self.betas_sm = self.resultado.params
    print("Parámetros de la regresión:")
    print("b0:", self.betas_sm[0])
    print("b1:", self.betas_sm[1])

  def calcular_varianza_error_sm(self): #Debería ser igual que se_sm
    # Calcula la varianza del error
    self.varianza_error_sm = self.resultado.mse_resid
    print("Varianza del error:", self.varianza_error_sm)

  def calcular_r2_sm(self):
    # Calcula el coeficiente de determinación
    self.r2_sm = self.resultado.rsquared
    print("Coeficiente de determinación:", self.r2_sm)

  def calcular_se_betas_sm(self):
    # Calcula los desvíos standard para b0 y b1
    self.se_betas_sm = self.resultado.bse
    print("Errores estándar de los coeficientes:")
    print("SE(b0.est):", self.se_betas_sm[0])
    print("SE(b1.est):", self.se_betas_sm[1])

  def calcular_p_valores_sm(self):
    # Calcula el p_valor para H0: bi = 0.
    self.p_valores_sm = self.resultado.pvalues
    print("P-valores de los coeficientes:")
    print("P-valor(b0.est):", self.p_valores_sm[0])
    print("P-valor(b1.est):", self.p_valores_sm[1])

  def calcular_int_conf_betas_sm(self, alfa=0.05):
    # Calcula el intervalo de confianza para b0 y b1
    self.int_conf_betas_sm = self.resultado.conf_int(alpha=alfa)
    print("Intervalo de confianza para b0.est:", self.int_conf_betas_sm[0])
    print("Intervalo de confianza para b1.est:", self.int_conf_betas_sm[1])

  def calcular_t_obs_b0_sm(self, b0_propuesto = 0):
    # Calcula t_obs para b0 bajo H0: b0 = b0_propuesto, por DEFECTO = 0.
    self.t_obs_b0_sm = (self.betas_sm[0]-b0_propuesto)/self.se_betas_sm[0]
    print("t_obs(b0.est):", self.t_obs_b0_sm)
    #return self.t_obs_b0_sm

  def calcular_t_obs_b1_sm(self, b1_propuesto = 0):
    # Calcula t_obs para b1 bajo H0: b1 = b1_propuesto, por DEFECTO = 0.
    self.t_obs_b1_sm = (self.betas_sm[1]-b1_propuesto)/self.se_betas_sm[1]
    print("t_obs(b1.est):", self.t_obs_b1_sm)
    #return self.t_obs_b1_sm

  def calcular_int_conf_pred_nuevo_x_sm(self, x_new, alfa=0.05, int_pred=False):
    # Nuevo punto de predicción
    self.x_new = x_new

    # Respuesta estimada para x_new
    self.y_new = self.betas_sm[0] + self.betas_sm[1] * self.x_new
    print("Respuesta estimada para x_new:", self.y_new)

    # Crear la matriz de diseño con el nuevo punto de predicción
    X_new = sm.add_constant(np.array([[1, self.x_new]]))

    # Obtener el intervalo de predicción para el nuevo punto
    prediccion = self.resultado.get_prediction(X_new)
    self.int_conf = prediccion.conf_int(alpha =alfa)
    print("Intervalo de confianza para la respuesta MEDIA a UN valor nuevo:", self.int_conf)
    if int_pred:
      self.int_pred = prediccion.conf_int(obs=True, alpha=alfa)
      print("Intervalo de predicción para la respuesta a UN valor nuevo:", self.int_pred)

"""# 4. Regresión Lineal Múltiple como clase de clase RegresiónLineal"""

# 07-05-25
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

class RegresionLineal:
    def __init__(self, x, y):
        # x = variables predictora/s
        # y = variable respuesta
        self.x = x
        self.y = y

    def mostrar_estadisticas(self):
        # Completar
        # Muestra estadísticas de y solamente
        n = len(self.y)
        media_y = sum(self.y)/n
        varianza_y = sum((self.y-media_y)**2/(n-1))
        min_y = min(self.y)
        max_y = max(self.y)
        print(f'Cantidad de datos: {n}.')
        print(f'Media de y       : {media_y}.')
        print(f'Varianza de y    : {varianza_y}.')
        print(f'Desvío std de y  : {varianza_y**0.5}')
        print(f'Rango de y       : [{min_y},{max_y}]')


    def evalua_histograma(self, h, grilla_x):
        # Completar
        # Utiliza genera_histograma de y, con ventana h,  como función de densidad para estimar las probabilidades en grilla_x.

        def genera_histograma(h):
        # Genera histograma de y
        # Necesita h (ventana).
        # Retorna puntos separadores de bins y recuento de cada bin.


          bins = np.arange(min(self.y), max(self.y)+h,h)
          histo = np.zeros(len(bins)-1)
          for dato in self.y:
            for i in range(len(histo)):
              if bins[i] <= dato < bins[i+1]:
                histo[i] +=1
                break
          histo /= (len(self.y) * h)
          return bins, histo

        bins, frec = genera_histograma(h)
        estim_hist = np.zeros(len(grilla_x))
        for j in range(len(grilla_x)):
          for i in range(len(bins)-1):
              if bins[i] <= grilla_x[j] < bins[i+1]:
                estim_hist[j] = frec[i]
                break
        return estim_hist

    def ajustar_modelo_sm(self):
        # Completar
        # Estimo modelo de regresión lineal
        # Genero result, ajustados y resumen (diccionario)
        self.X = sm.add_constant(self.x)
        self.modelo_sm = sm.OLS(self.y, self.X)
        self.result_sm = self.modelo_sm.fit()
        self.ajustados = np.dot(self.X, self.result_sm.params)
        # podría usar result_sm.fittedvalues, es lo mismo

        self.resmuen_ajuste_sm ={'betas': self.result_sm.params,
                              'p_valores': self.result_sm.pvalues,
                              'predichos': self.ajustados,
                              'r2_ajustado': self.result_sm.rsquared_adj,
                              'resultado': self.result_sm,
                              'summary': self.result_sm.summary()}
        return self.resmuen_ajuste_sm

class RegresionLinealSimple(RegresionLineal):
    def __init__(self, x, y):
        super().__init__(x, y)

        #Si quiero pasar los labels:
        #def __init__(self,x,y,label_x='x,label_y='y') ????
        #    super().__init__(x,y)                    ????
        # o van en class RegresionLinealSimple(RegresionLineal,label_x='x,label_y='y')                         ????????

    def predecir(self, new_x, alfa=0.05):
        # Completar
        new_X = np.array([1,new_x])
        self.y_nuevo = np.dot(new_X, self.result_sm.params)
        return self.y_nuevo

    def predecir_sm(self, new_x, alfa = 0.05):
        # Completar
        #super().ajustar_modelo_sm()

        new_X = np.array([1,new_x])


        self.prediccion_sm = self.result_sm.get_prediction(new_X, alpha=alfa)

        self.y_nuevo_sm = self.prediccion_sm.predicted_mean

        self.intervalo_confianza_y_nuevo_sm = self.prediccion_sm.conf_int(alpha=alfa)

        self.intervalo_prediccion_y_nuevo_sm = self.prediccion_sm.conf_int(obs=True,alpha=alfa)

        return self.y_nuevo_sm, self.intervalo_confianza_y_nuevo_sm, self.intervalo_prediccion_y_nuevo_sm

    def graficar_recta_ajustada(self, label_x='x', label_y='y'):
        # Completar

      # Grafica los puntos de los datos originales y la recta de mejor ajuste por mínimos cuadrados obtenida por regresión lineal simple.
      self.x_label = label_x
      self.y_label = label_y

      resumen_ajuste = super().ajustar_modelo_sm()
      self.y_estimada = resumen_ajuste['predichos']

      plt.figure(figsize=(10, 6))
      plt.scatter(self.x, self.y, label="Datos")
      plt.plot(self.x, self.y_estimada, label="Recta Estimada", linestyle="--", color="green")
      plt.xlabel(self.x_label)
      plt.ylabel(self.y_label)
      plt.title("Gráfico de Dispersión con Recta de Mejor Ajuste")
      plt.legend()

class RegresionLinealMultiple(RegresionLineal):
    def __init__(self, x, y):
        super().__init__(x, y)

    def predecir(self, new_x):
        # Completar
        pass

    def predecir_sm(self, new_x, alfa=0.05):
        # Completar
        #new_x debe ser np.array([1,valores])

        self.prediccion_sm = self.result_sm.get_prediction(new_x)

        self.y_nuevo_sm = self.prediccion_sm.predicted_mean

        self.intervalo_confianza_y_nuevo_sm = self.prediccion_sm.conf_int(alpha=alfa)

        self.intervalo_prediccion_y_nuevo_sm = self.prediccion_sm.conf_int(obs=True,alpha=alfa)

        return self.y_nuevo_sm, self.intervalo_confianza_y_nuevo_sm, self.intervalo_prediccion_y_nuevo_sm

#### NO


import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

class RegresionLineal:
    def __init__(self, x, y):
        # x = variables predictora/s
        # y = variable respuesta
        self.x = x
        self.y = y

    def mostrar_estadisticas(self):
        # Completar
        # Muestra estadísticas de y solamente
        n = len(self.y)
        media_y = sum(self.y)/n
        varianza_y = (self.y-media_y)**2/(n-1)
        min_y = min(self.y)
        max_y = max(self.y)
        print(f'Cantidad de datos: {n}.')
        print(f'Media de y       : {media_y}.')
        print(f'Varianza de y    : {varianza_y}.')
        print(f'Desvío std de y  : {varianza_y**0.5}')
        print(f'Rango de y       : [{min_y},{max_y}]')


    def evalua_histograma(self, h, grilla_x):
        # Completar
        # Utiliza genera_histograma de y, con ventana h,  como función de densidad para estimar las probabilidades en x.

        def genera_histograma(self, h):
        # Genera histograma de y
        # Necesita h (ventana).
        # Retorna puntos separadores de bins y recuento de cada bin.


        bins = np.arange(min(self.y), max(self.y)+h,h)
        histo = np.zeros(len(bins)-1)
        for dato in self.y:
          for i in range(len(histo)):
            if bins[i] <= dato < bins[i+1]:
              histo[i] +=1
              break
        histo /= (len(self.y) * h)
        return bins, histo

      bins, frec = self.genera_histograma(h)
      estim_hist = np.zeros(len(grilla_x))
      for j in range(len(grilla_x)):
        for i in range(len(bins)-1):
            if bins[i] <= x[j] < bins[i+1]:
              estim_hist[j] = frec[i]
              break
      return estim_hist

    def ajustar_modelo_sm(self):
        # Completar
        # Estimo modelo de regresión lineal
        # Genero result, ajustados y resumen (diccionario)
        self.X = sm.add_constant(self.x)
        self.modelo_sm = sm.OLS(self.y, self.X)
        self.result_sm = self.modelo_sm.fit()
        self.ajustados = np.dot(self.X, self._sm.params)
         # podría usar result_sm.fittedvalues, es lo mismo

        self.resmuen_ajuste_sm ={'betas': self.result_sm.params,
                              'p_valores': self.result_sm.p_values,
                              'predichos': self.result_sm.fittedvalues,
                              'r2_ajustado': self.result_sm.rsquared_adj,
                              'resultado': self.result_sm,
                              'summary': self.result_sm.summary()}
        return self.resmuen_ajuste_sm

class RegresionLinealSimple(RegresionLineal):
    def __init__(self, x, y):
        super().__init__(x, y)

        #Si quiero pasar los labels:
        #def __init__(self,x,y,label_x='x,label_y='y') ????
        #    super().__init__(x,y)                    ????
        # o van en class RegresionLinealSimple(RegresionLineal,label_x='x,label_y='y')                         ????????

    def predecir(self, new_x, alfa=0.05):
        # Completar
        new_X = sm.add_constant(new_x)
        self.y_nuevo = np.dot(new_X, self.result.params)
        return self.y_nuevo

    def predecir_sm(self, new_x, alfa = 0.05):
        # Completar
        super().ajustar_modelo_sm()

        new_X = sm.add_constant(new_x)
        self.prediccion_sm = self.result_sm.get_prediction(new_X, alpha=alfa)
        self.y_nuevo_sm = self.prediccion_sm.predicted_mean

        self.intervalo_confianza_y_nuevo_sm = self.prediccion_sm.conf_int(alpha=alfa)

        self.intervalo_prediccion_y_nuevo_sm = self.prediccion_sm.conf_int(obs=True,alpha=alfa)

        return self.y_nuevo_sm, self.intervalo_confianza_y_nuevo_sm, self.intervalo_prediccion_y_nuevo_sm

    def graficar_recta_ajustada(self, label_x='x', label_y='y'):
        # Completar

      # Grafica los puntos de los datos originales y la recta de mejor ajuste por mínimos cuadrados obtenida por regresión lineal simple.
      self.x_label = label_x
      self.y_label = label_y

      resumen_ajuste = super().ajustar_modelo_sm()
      self.y_estimada = self.resumene_ajuste_sm['predichos']

      plt.figure(figsize=(10, 6))
      plt.scatter(self.x, self.y, label="Datos")
      plt.plot(self.x, self.y_estimada, label="Recta Estimada", linestyle="--", color="green")
      plt.xlabel(self.x_label)
      plt.ylabel(self.y_label)
      plt.title("Gráfico de Dispersión con Recta de Mejor Ajuste")
      plt.legend(

class RegresionLinealMultiple(RegresionLineal):
    def __init__(self, x, y):
        super().__init__(x, y)

    def predecir_y(self, new_x):
        # Completar
        pass

    def predecir_sm(self, new_x, alfa=0.05):
        # Completar
        super().ajustar_modelo_sm()

        new_X = sm.add_constant(new_x)
        self.prediccion_sm = self.result_sm.get_prediction(new_X)
        self.y_nuevo_sm = self.prediccion_sm.predicted_mean

        self.intervalo_confianza_y_nuevo_sm = self.prediccion_sm.conf_int(alpha=alfa)

        self.intervalo_prediccion_y_nuevo_sm = self.prediccion_sm.conf_int(obs=True,alpha=alfa)

        return self.y_nuevo_sm, self.intervalo_confianza_y_nuevo_sm, self.intervalo_prediccion_y_nuevo_sm

"""#5. Clase Separacion_train_test"""

import random

class Separacion_train_test():
  ''' Separa datos en datos_train y datos_test
      Argumento: datos

      Método:
          particion(p_train,seed)
                p_train: [0,1] proporción de datos para train: ej: 0.8
                seed: valor de la semilla para la elección random: defecto=10
      Ejemplo de uso:
          datos_train,datos_test = Separacion_train_test(datos).particion(0.8,10)
  '''

  def __init__(self,datos):
    self.datos = datos


  def particion(self,p_train,seed=10):
    n = self.datos.shape[0]
    n_train = int(n*p_train)
    n_test = n - n_train

    random.seed(seed)
    cuales = random.sample(range(n), n_train)
    datos_test = datos.drop(cuales)
    datos_train = datos.iloc[cuales]
    return datos_train, datos_test

"""#6. Clase Regresión Logística"""

# 02-06-25
import numpy as np

import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn.metrics import auc
import pandas as pd
import random


class RegresionLogistica():
  ''' Clase para regresión logística.

    Métodos:
      calcular_logit(datos_y, datos_x)
      entrenar_logit(datos_y_train, datos_x_train)
      testear_logit(datos_y_test, datos_x_test)
      evaluar_modelo(umbral)
      imprimir_evaluacion_modelo()
      graficar_roc(p_min=0,p_max=1,num_puntos=100)

    # Ejemplo de uso.
      # 1. Separo datos con la clase Separacion_train_test
      datos_train,datos_test = Separacion_train_test(datos).particion(0.8,10)

      #2. Entreno modelo
      modelo = RegresionLogistica()
      resultado = modelo.entrenar_logit(y_train, x_train)

      #3. Testeo modelo
      resultado_testeo = modelo.testear_logit(y_test, x_test)

      #4. Tabla de confusión, sensibilidad y especificidad
      evaluacion = modelo.evaluar_modelo(0.5)
      modelo.imprimir_evaluacion_modelo()

      #5. Roc y auc
      modelo.graficar_roc()
  '''

  def __init__(self):
      self.resultado = None


  def calcular_logit(self, datos_y, datos_x):
    # Calcula los atributos esenciales del modelo de regresión logística.
    self.datos_y = datos_y
    self.datos_x = datos_x
    self.X = sm.add_constant(self.datos_x)
    modelo = sm.Logit(self.datos_y, self.X)
    self.resultado = modelo.fit()
    self.summary = self.resultado.summary()
    print(self.summary)

    # Calcula betas
    self.betas = self.resultado.params

    # Calcula el pseudo coeficiente de determinación
    self.pseudor2 = self.resultado.prsquared

    # Calcula el p_valor para H0: bi = 0.
    self.p_valores_betas = self.resultado.pvalues

    # Calcula los y_estimados por el modelo
    self.y_estimados = self.resultado.fittedvalues

    self.resumen_regresion = {
              'resultado':self.resultado,
              'betas':self.betas,
              'pseudor2':self.pseudor2,
              'p_valores_betas':self.p_valores_betas,
              'y_estimados':self.y_estimados,
              'summary':self.summary}
    return self.resumen_regresion


  def entrenar_logit(self,datos_y_train, datos_x_train):
    # Genero el modelo entrenado
    self.resultado_train = self.calcular_logit(datos_y_train, datos_x_train)
    return self.resultado_train


  def testear_logit(self,datos_y_test, datos_x_test):
    # Genero el testeo del modelo entrenado
    if self.resultado_train is None:
      print('No se ha entrenado el modelo.')
      return
    else:
      self.datos_y_test = datos_y_test
      self.datos_x_test = datos_x_test
      X_test = sm.add_constant(self.datos_x_test)
      self.predicciones = self.resultado_train['resultado'].predict(X_test)


  def evaluar_modelo(self, umbral):
    '''
    Evalua el modelo entrenado.
    Dado umbral, crea matriz de confusión, calcula error de clasificación, sensibilidad y especificidad.
    '''

    if self.predicciones is None:
      print('No se ha testeado el modelo.')
      return
    else:
      y_pred = (self.predicciones >= umbral).astype(int) # vector binario: 1 si es mayor a umbral

      #Matriz de confusión para umbral
      tabla =  pd.DataFrame({
          'y_test=1': [sum((y_pred == 1) & (self.datos_y_test == 1)), sum((y_pred == 0) & (self.datos_y_test == 1))],
          'y_test=0': [sum((y_pred == 1) & (self.datos_y_test == 0)), sum((y_pred == 0) & (self.datos_y_test == 0))],

      }, index=['y_pred=1', 'y_pred=0'])

      # Error de clasificación
      mal_clasificados = sum((y_pred == 1) & (self.datos_y_test == 0)) + sum((y_pred == 0) & (self.datos_y_test == 1))
      total = len(y_pred)
      error = mal_clasificados/total

      # Sensibilidad y especificidad
      sensibilidad = sum((y_pred == 1) & (self.datos_y_test == 1)) / sum(self.datos_y_test == 1)
      especificidad = sum((y_pred == 0) & (self.datos_y_test == 0)) / sum(self.datos_y_test == 0)

      self.evaluacion_modelo = {
          'tabla':tabla,
          'error':error,
          'sensibilidad':sensibilidad,
          'especificidad':especificidad
      }
      return self.evaluacion_modelo


  def imprimir_evaluacion_modelo(self):
    # Imprime resultados de la evaluación: tabla confusión, error, sens y espec

    if self.evaluacion_modelo is None:
      print('No se ha evaluado el modelo.')
      return
    else:
      print('Matriz de confusión:')
      print(self.evaluacion_modelo['tabla'])
      print('Error de clasificación:', self.evaluacion_modelo['error'])
      print('Sensibilidad (positivos correctos detectados/total que debería haber detectado (y_test=1):', self.evaluacion_modelo['sensibilidad'])
      print('Especificidad (negativos correctos/total de negativos que debería haber detectado (y_test=0):', self.evaluacion_modelo['especificidad'])


  def graficar_roc(self,p_min=0,p_max=1,num_puntos=100):
    # Grafica ROC, calcula p de corte óptimo, sensibilidad y especificidad.

    if self.predicciones is None:
      print('No se ha testeado el modelo.')
      return
    else:
    # Generar valores de p
      p_values = np.linspace(p_min, p_max, num_puntos)

    # Listas para almacenar sensibilidad y especificidad
    sensibilidad = []
    especificidad = []
    for p in p_values:
        # Calcular matriz de confusión
        confusion_p = self.evaluar_modelo(p)['tabla']
        a = confusion_p['y_test=1']['y_pred=1']
        b = confusion_p['y_test=0']['y_pred=1']
        c = confusion_p['y_test=1']['y_pred=0']
        d = confusion_p['y_test=0']['y_pred=0']
        # Calcular sensibilidad y especificidad
        sensibilidad.append(a / (a + c))
        especificidad.append(d / (b + d))

    # Punto de corte, sensibilidad y especificidad
    idx_umbral_p = np.argmax(np.array(sensibilidad)+np.array(especificidad)-1)
    umbral_p = p_values[idx_umbral_p]
    sensibilidad_p = sensibilidad[idx_umbral_p]
    especificidad_p = especificidad[idx_umbral_p]

    # AUC
    roc_auc = auc(1-np.array(especificidad), sensibilidad)

    # Graficar ROC
    plt.plot(1-np.array(especificidad), sensibilidad)
    plt.xlabel('1-especificidad')
    plt.ylabel('sensibilidad')
    plt.title('Curva ROC')
    plt.grid(True)
    plt.show()
    print('Umbral p:', umbral_p)
    print('Sensibilidad:', sensibilidad_p)
    print('Especificidad:', especificidad_p)
    print("AUC:", roc_auc)

"""# Otros"""

from google.colab import drive
import pandas as pd

# Lee el archivo CSV
drive.mount('/content/drive')
camino = '/content/drive/MyDrive/Colab Notebooks/CD3 2025/Datos/'

datos= pd.read_excel(camino+"autos.xlsx")
datos= pd.read_csv(camino+'salary.csv")

df = pd.DataFrame(datos)

# Tipo de auto es categórica, con 3 niveles. Creo 2 dummy
x1_d=pd.get_dummies(x1,drop_first=True)
x1_d.columns=['van','SUV']

# las dummy son x1_d['van] y x1_d['SUV'], dejo 'sedan' en intercepto.

import matplotlib.pyplot as plt

predictoras = ['Water.Temp', 'Acid.Conc.', 'Air.Flow']
respuesta = 'stack.loss'
plt.figure(figsize=(10, 6))
for i, predictora in enumerate(predictoras, 1):
    plt.subplot(1, 3, i)
    plt.scatter(df[predictora], df[respuesta])
    plt.title(f'{predictora} vs. {respuesta}')

#1. Media de residuos =0
print(f'Media de los residuos: {resultado_3["resultado"].resid.mean()}')

#2. Distrib normal de los residuos
sns.histplot(resultado_3["resultado"].resid)
#sns.histplot(modelo.result_sm.resid)
##qqplot
plt.figure()
sm.qqplot(resultado_3["resultado"].resid, line='s')
#sm.qqplot(modelo.result_sm.resid, line='s')
plt.show()

"""**RTA.: Pareciera no haber normalidad**"""

# Test de normalidad de residuos - Shapiro
from scipy.stats import shapiro
shapiro = shapiro(resultado_3["resultado"].resid)
print(f'Estadístico de prueba Shapiro: {shapiro.statistic}')
print(f'Valor p Shapiro: {shapiro.pvalue}')
if shapiro.pvalue > 0.05:
    print("Los residuos siguen una distribución normal.")
else:
    print("Los residuos no siguen una distribución normal.")

#Independencia de residuos: Durbin
from statsmodels.stats.stattools import durbin_watson
durbin = durbin_watson(resultado_3["resultado"].resid)
print(f'Durbin-Watson (0-4): {durbin}')
if durbin < 1.5:
    print("Los residuos están correlecionados +.")
elif durbin >2.5:
    print("Los residuos están correlacionados -.")
else:
    print("Los residuos no están correlacionados.")

#. Homocedasticidad: dispersión de residuos
#. Linealidad del modelo
#sns.scatterplot(x=modelo_3.result_sm.fittedvalues,y=modelo_3.result_sm.resid)
sns.scatterplot(x=resultado_3['predichos'],y=resultado_3["resultado"].resid)

"""**Rta.:
- linealidad: la nube está alrededor del 0. OK.
- homoceda: pareciera haber embudo, no serían varianzas iguales. NO OK**
"""

# Modelo de Regresión Lineal Múltiple
# y = b0 + b1*x1_d['van'] + b2*x1_d['SUV'] + b3*x2 + b4*x3
x_modelo_completo=np.stack((x1_d['van'],x1_d['SUV'],x2,x3),axis=1)
modelo_completo=RegresionLinealMultiple(x=x_modelo_completo,y=y)
resultado_completo = modelo_completo.ajustar_modelo_sm()

"""## 3. Estimá los coeficientes de regresión."""

resultado_completo['betas']

"""##4. Realizá el análisis de los residuos ¿Se verifican los supuestos en este caso?"""

import seaborn as sns
residuos_completo = resultado_completo['resultado'].resid
predichos_completo = resultado_completo['predichos']
sns.scatterplot(x=predichos_completo,y=residuos_completo)

"""**RTA.: Los residuos están clasificados en 2 categorías, una alrededor del 20 y la otra alrededor del 32.**

## 5. Realizá una prueba de hipótesis para cada coeficiente y calculá el valor p asociado a dicha prueba. ¿Están todas las variables predictoras asociadas al gasto de combustible?
"""

# H0: b_i = 0, H_1: b_i != 0, i=0,4
#Informe de los betas
for i in range(0,len(resultado_completo['p_valores'])):
  if resultado_completo['p_valores'].values[i]<0.05:
    print(f'b{i} es significativo, p-valor:',resultado_completo['p_valores'].values[i])
  else:
    print(f'b{i} NO es significativo, p-valor:',resultado_completo['p_valores'].values[i])
print(f'R2-Ajustado:', {resultado_completo['r2_ajustado']})

"""**RTA.: No, la variable x3 = 'Octanaje' no está asociada al consumo. La variable Tipo_de_auto sí, porque un tipo de auto da significativo.**

## 6.Realizá una estimación del modelo utilizando las variables cuyos coeficientes de valor-p es menor a 0.05. ¿Explica esta nueva estimación la variación del gasto de combustible, con el mismo error que la que utiliza todas las variables? para responder esta pregunta podríamos utilizar el siguiente código:
"""

x_modelo_reducido = np.stack((x1_d['van'],x1_d['SUV'],x2),axis=1)
modelo_reducido=RegresionLinealMultiple(x=x_modelo_reducido,y=y)
resultado_reducido = modelo_reducido.ajustar_modelo_sm()

plt.plot(x2[x1==1], ajustados3[x1==1], linestyle='--', color='blue')
plt.plot(x2[x1==0], ajustados3[x1==0], linestyle='--', color='green')

"""Por último, podemos realizar un test para decidir si el coeficiente de la "nueva variable" es significativamente distinto de cero o no, es decir, si la diferencia de pendientes que se observa en las rectas es significativa.

Las hipótesis, según la notación de arriba, a testear son

H$_0:\beta_3=0$ versus H$_1:\beta_3\neq 0$

Informá el $p-$valor y concluí sobre las pendientes

✍ Lugar para que respondas
"""

## Completar
print(result3.summary())

"""**RTA.: el p-valor de b3=0.664, grande >0.05 => acepto HO => el b3 es NO significativo => la interacción no aporta información**

## ▶ Qué aprendimos con este ejercicio? 🚩

El modelo que acabamos de presentar
$$Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3 X_1X_2+\varepsilon$$
donde
$Y$: salario (salary),
$X_1$: sexo del docente (sex),
$X_2$: años del docente en el cargo actual (year),
incluye el término $\beta_3 X_1X_2$ que corresponde a la posible **interacción** entre las variables $X_1$ y $X_2$.

En el modelo sin interacción (de la clase pasada) podés afirmar cuánto varía la respuesta (`salary`) por cada año de antigüedad (`year`) sin saber el valor de la otra variable (`sex`).  Cuando hay interacción (como la que se presentó hoy), esta interpretación se dificulta.

Una interacción ocurre cuando una variable predictora tiene un efecto diferente sobre la respuesta dependiendo del valor de otra variable predictora.
En el caso que estudiamos una variable predictora es cuantitativa (`years`) y la otra es cualitativa (`sex`). Sin embargo, se pueden incluir en el modelo interacciones entre cualquier tipo de variables.

Las interacciones de variables predictoras deben ser consideradas en el modelo cuando se tenga alguna sospecha de que alguna tiene un efecto distinto sobre la respuesta dependiento el valor que tome la otra predictora.

##**Ejercicio**
##Esto es una competencia!! 🏅

1. Seleccioná al azar 10 datos de `salary.csv` y descartá estos datos, es decir, ahora tu conjunto de datos tendrá 42 filas.  Ahora tu conjunto de datos se llama `datos_train`.

Además creá el conjunto de datos que eliminaste y llamalo `datos_test`, este conjunto de datos tendrá 10 filas.

Para hacer esto podés ayudarte con el siguiente código.

👀!!! para los items 2 y 3 que siguen usá `datos_train`.
"""

datos['degree'].unique()

datos['rank'].unique()

#Codifico 'degree': 0=Master, 1=PhD
x3=1*(datos['degree']=='Masters')

x3.unique()

import random
random.seed(10) # fijamos la semilla para que no haya "favoritismos" en la competencia

# Crear un vector de 10 números aleatorios entre 0 y 52
cuales = random.sample(range(51), 10)

datos_train = datos.drop(cuales)
print(datos_train.shape)

#print(cuales) #para chequear
datos_test = datos.iloc[cuales]
#print(datos_test) #para chequear

"""2. Considerá modelos que contengan todas o un subconjunto de las variables disponibles como predictoras: `degree`, `rank`, `sex`, `year` y `ysdeg` (podés considerar interacciones)."""

## Completar
# separo las columnas en variables diferentes:
y_train=datos_train['salary']
x1_train=1*(datos_train['sex']=='Female')
x2_train=datos_train['year']
x3_train=1*(datos_train['degree']=='Masters')
x4_train=datos_train['rank']
x5_train=datos_train['ysdeg']

#RLM 1

# Hago test de hipotesis: H0: media_male =  media_female, H1: media_male > media_female  (para ver si los hombres ganan más)

t_obs_mayor, p_value = stats.ttest_ind(datos_male['salary'], datos_female['salary'], alternative='greater')

print(f"Estadístico t (unilateral-mayor): {t_obs_mayor}")
print(f"Valor p (unilateral-mayor): {p_value}")


if p_value < alpha:
    print("Se rechaza la hipótesis nula. Existe evidencia estadística para concluir que el salario medio de los hombres es mayor al de las mujeres.")
else:
    print("No se rechaza la hipótesis nula. No hay suficiente evidencia estadística para concluir que el salario medio de los hombres es mayor al de las mujeres.")

# Estadístico para diferencia de medias
import numpy as np
from scipy.stats import t

alfa = 0.05
salarios_h = datos_male['salary'].to_numpy()
salarios_f = datos_female['salary'].to_numpy()

# Tamaños muestrales
n_m = len(salarios_h)
n_f = len(salarios_f)

# Medias de cada muestra
media_male = np.mean(salarios_h)
media_female = np.mean(salarios_f)

# Varianzas. OJO: numpy hace /n, no /(n-1)
s2_m = np.var(salarios_h)
s2_f = np.var(salarios_f)

# Desviación estándar combinada (sp)
# OJO: con n y no (n-1) porque numpy da s2 = /n, no /(n-1).
numerador_sp = (n_m )* s2_m + (n_f )* s2_f
denominador_sp =  n_m + n_f - 2
sp = np.sqrt(numerador_sp / denominador_sp)

# Estadístico de la diferencia de medias
estadistico_t = (media_male - media_female) / (sp * np.sqrt((1/n_m)+(1/n_f)))
print(f'estadistico_t: {estadistico_t}')


# p-valor utilizando la distribución t (unilateral)
p_valor_unilat =  (1 - t.cdf(estadistico_t, n_m+n_f-2))
print(f'p_valor (unilateral): {p_valor_unilat}')

if p_valor_unilat < alfa:
    print("Se rechaza la hipótesis nula. Existe evidencia estadística para concluir que el salario medio de los hombres es mayor al de las mujeres.")
else:
    print("No se rechaza la hipótesis nula. No hay suficiente evidencia estadística para concluir que el salario medio de los hombres es mayor al de las mujeres.")

"""✍ Lugar para que respondas

**RTA.: rechazo H0.

La media de los salarios de los varones es MAYOR que la de las mujeres.**
"""

y=datos['salary']
x=datos['sex']
x=1*(datos['sex']=='Female')

"""**RTA.: Un array con 0,1, según sea male(0) o female(1).

Obtené el $p-$valor para concluir sobre las hipótesis

$H_0:\beta_1=0$ versus $H_0:\beta_1\neq 0$.
"""

## Completar
regresion_salario = RegresionLinealSimple(x,y)
regresion_salario.calcular_regresion_sm()
regresion_salario.calcular_t_obs_b1_sm()
regresion_salario.calcular_t_critico()

## Completar
x0=np.array([1,0])
y_estimada10 = regresion_salario.b0 + regresion_salario.b1 * x0
print(y_estimada10)

## Completar
x_year_female=datos['year'][datos['sex']=='Female']
y_female=datos['salary'][datos['sex']=='Female']

x_year_male=datos['year'][datos['sex']=='Male']
y_male=datos['salary'][datos['sex']=='Male']

regresion_salario_female = RegresionLinealSimple(x_year_female,y_female)
regresion_salario_female.calcular_regresion_sm()
regresion_salario_female.grafico_scatter_y_lineal()

from statsmodels.stats.anova import anova_lm

# Realizar la comparación ANOVA
anova_resultado = anova_lm(resultado_reducido['resultado'], resultado_completo['resultado'])

# Mostrar la tabla ANOVA
print(anova_resultado)

#Interpretación del anova
if anova_resultado['Pr(>F)'][1] < 0.05:
    print("El modelo completo mejora significativamente.")
else:
    print("El modelo completo NO mejora significativamente.")


# AGREGO COMENTARIO PARA modificar archivo en nueva_rama de c:/ussers/sebag/mimodulo y luego subirlo a git hub 05-06-25